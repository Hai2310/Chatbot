import gradio as gr
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch

# =========================
# Load local model
# =========================
MODEL_NAME = "Qwen/Qwen2.5-1.5B-Instruct"

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto"
)

generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
)

# =========================
# System prompt
# =========================
SYSTEM_PROMPT = """
B·∫°n l√† m·ªôt chuy√™n gia t∆∞ v·∫•n t√¢m l√Ω.
Gi·ªçng n√≥i nh·∫π nh√†ng, t√¥n tr·ªçng, kh√¥ng ph√°n x√©t.
Kh√¥ng ƒë∆∞a ra ch·∫©n ƒëo√°n y khoa.
Lu√¥n khuy·∫øn kh√≠ch ng∆∞·ªùi d√πng chia s·∫ª c·∫£m x√∫c.
"""

# =========================
# Chat function
# =========================
def stream_response(message, history):
    prompt = SYSTEM_PROMPT + "\n"

    for user, bot in history:
        prompt += f"Ng∆∞·ªùi d√πng: {user}\nTr·ª£ l√Ω: {bot}\n"

    prompt += f"Ng∆∞·ªùi d√πng: {message}\nTr·ª£ l√Ω:"

    output = generator(
        prompt,
        max_new_tokens=300,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
        repetition_penalty=1.1
    )

    text = output[0]["generated_text"]
    answer = text[len(prompt):].strip()

    # streaming gi·∫£
    partial = ""
    for ch in answer:
        partial += ch
        yield partial


# =========================
# Gradio UI
# =========================
demo = gr.ChatInterface(
    fn=stream_response,
    textbox=gr.Textbox(
        placeholder="B·∫°n ƒëang c·∫£m th·∫•y th·∫ø n√†o?",
        container=False,
        scale=7
    ),
    title="üß† Chatbot T∆∞ V·∫•n T√¢m L√Ω (Local)",
    description="Ch·∫°y ho√†n to√†n tr√™n m√°y ‚Äì kh√¥ng c·∫ßn Internet",
)

demo.launch()
